<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hugo Thimonier</title>

    <link rel='icon' href='./content/media/logo300.png' type='image/png'>
    <link href="./css/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.1/css/all.css" integrity="sha384-50oBUHEmvpQ+1lW4y57PTFmhCaXp0ML5d60M1M7uH2+nqUivzIebhndOJK28anvf" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
    <script async defer src="https://buttons.github.io/buttons.js"></script>
</head>

<body>

<!-- Navigation bar -->
<div class="navbar navbar-default  navbar-fixed-top bg-info">
    <div class="container">
        <div class="navbar-header">

            <button class="navbar-toggle" type="button" data-toggle="collapse" data-target="#navbar-main">
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
        </div>
        <div class="navbar-collapse collapse" id="navbar-main">

            <ul class="nav navbar-nav navbar-left">
                <li><a href="#news">News</a></li>
                <li><a href="#research">Research</a></li>
            </ul>
        </div>
    </div>
</div>

<!-- end of navigation bar -->

<!-- <div style="height:80px;"></div>
<div style="height:40px;"></div> -->

<!-- CONTENTS -->
<main id="main-content" class="container">
    <!-- Aboutme -->
    <!--<div id="aboutme"></div> -->
    <div class="row">
        <div class="col-md-3">
            <a class="thumbnail">
                <img src="./content/media/thumbnail.png"
                     alt="Hugo Thimonier" class="picture" style="max-width: 80%; height: auto;  border-radius: 
                     50%; display: block; margin: 0 auto;" >
            </a>
        </div>

        <div class="col-md-9">
            <h1 class="text-info">Hugo Thimonier, PhD</h1>
            <h4 class="text-info">ML Research Scientist, Emobot</h4>
            <h5>
                <!--Email Me--> 
                <a href="mailto:thimonier.hugo@gmail.com" class="text-info" title="e-Mail"><i
                        class="fa fa-envelope-square fa-2x"></i></a>
                <!--Google scholar--> 
                <a 
                    href="https://scholar.google.com/citations?user=p1mWlucAAAAJ&hl=fr&oi=ao" 
                    class="text-info"
                    title="Google Scholar"
                    rel="noopener noreferrer"
                    target="_blank">
                    <i 
                        class="ai ai-google-scholar-square ai-2x">
                    </i>
                </a>
                <!--arXiv--> 
                <a 
                    href="https://arxiv.org/search/cs?searchtype=author&query=Thimonier,+H" 
                    class="text-info"
                    title="ArXiv" 
                    target="_blank">
                    <i 
                        class="ai ai-arxiv-square ai-2x">
                    </i>
                </a>
                <!--GitHub--> 
                <a 
                    href="https://github.com/hugothimonier" 
                    target="_blank" 
                    class="text-info"
                    rel="noopener noreferrer"
                    title="GitHub">
                    <i
                        class="fab fa-github-square fa-2x">
                    </i>
                </a>
                <!--Linkedin--> 
                <a 
                    href="https://www.linkedin.com/in/hugo-thimonier-65949ba6/" 
                    class="text-info"
                    title="LinkedIn"
                    rel="noopener noreferrer"
                    target="_blank">
                    <i 
                        class="fab fa-linkedin fa-2x">
                    </i>
                </a>
                <!--CV--> 
                <a 
                    href="https://github.com/hugothimonier/hugothimonier.github.io/raw/master/content/cv/CV.pdf" 
                    rel="noopener noreferrer"
                    class="text-info"
                    title="Resume"
                    target="_blank">
                    <i 
                        class="fa fa-id-card fa-2x"
                        aria-hidden="true" 
                        >
                    </i>
                </a>
            </h5>

            <p align="justify">
                I am currently working as a ML Research Scientist at <a href="https://www.emobot.fr/" target="_blank" rel="noopener noreferrer">Emobot</a>. 
                I hold a PhD in Computer Science from 
                <a href="https://www.centralesupelec.fr/fr/le-centre-de-recherche-centralesupelec" target="_blank" rel="noopener noreferrer">CentraleSupélec</a> and the 
                <a href="https://www.lisn.upsaclay.fr/" target="_blank" rel="noopener noreferrer">LISN</a> Lab. 
                My current research interest ranges from (supervised) <b>Anomaly detection</b>, <b>Self-supervised Learning</b> (SSL),
                <b>Deep Learning for tabular data, audio and video</b>. 
                I have received an MSc degree in Statiscial Engineering from 
                <a href="https://www.ensae.fr/" target="_blank" rel="noopener noreferrer">ENSAE</a>  and studied 
                as a Normalien at <a href="https://ens-paris-saclay.fr/" target="_blank" rel="noopener noreferrer">ENS Paris-Saclay</a>.
            </p>
        </div>
    </div> 
    <!-- end of Aboutme -->

    <hr>

    <!-- News header-->
    <div class="row" id="news">
        <div class="col-md-12">
            <h2>News</h2>
        </div>
    </div>
    <!-- End news header -->

    <!-- EmoSLLM 2025-->

    <div class="row">
        <div class="col-xs-12 col-sm-2 col-md-2 date-column">
            <span class="label label-2025">12 / 2025</span>
        </div>
        <div class="col-xs-12 col-sm-10 col-md-10">
            We went to <a href="https://eurips.cc/" target="_blank" rel="noopener noreferrer">EurIPS</a> to present our latest work, <a href="https://arxiv.org/abs/2508.14130" target="_blank" rel="noopener noreferrer">EmoSLLM</a>, at the workshop on <a href="https://multimodal-rep-learning-for-health.github.io/" target="_blank" rel="noopener noreferrer">Multimodal Representation Learning for Healthcare</a>.
        </div>
    </div>

    <!-- EmoSLLM 2025-->

    <div class="row">
        <div class="col-xs-12 col-sm-2 col-md-2 date-column">
            <span class="label label-2025">08 / 2025</span>
        </div>
        <div class="col-xs-12 col-sm-10 col-md-10">
            Our latest work on LLM fine-tuning for SER, <a href="https://arxiv.org/abs/2508.14130" target="_blank" rel="noopener noreferrer">EmoSLLM: Parameter-Efficient Adaptation of LLMs for Speech Emotion Recognition</a> is
            now on arXiv!
        </div>
    </div>


    <!-- T-JEPA 2025-->

    <div class="row">
        <div class="col-xs-12 col-sm-2 col-md-2 date-column">
            <span class="label label-2025">01 / 2025</span>
        </div>
        <div class="col-xs-12 col-sm-10 col-md-10">
            Our latest work on SSL, <a href="https://arxiv.org/abs/2410.05016" target="_blank" rel="noopener noreferrer">T-JEPA: Augmentation Free Self-Supervised Learning for Tabular Data</a> has
            been accepted at <a href="https://iclr.cc/" target="_blank" rel="noopener noreferrer">ICLR 2025</a> !
        </div>
    </div>

    <!-- PhD 2024-->

    <div class="row">
        <div class="col-xs-12 col-sm-2 col-md-2 date-column">
            <span class="label label-2024">09 / 2024</span>
        </div>
        <div class="col-xs-12 col-sm-10 col-md-10">
            I have successfully defended my PhD in front of a jury composed of 
            <a href="https://homepages.laas.fr/louise/drupal/node/11" target="_blank" rel="noopener noreferrer">Louise Travé-Massuyès</a>,
            <a href="https://www.pantheonsorbonne.fr/page-perso/acelisse" target="_blank" rel="noopener noreferrer">Alain Celisse</a>, 
            <a href="https://ml.informatik.uni-kl.de/people/marius-kloft.php" target="_blank" rel="noopener noreferrer">Marius Kloft</a>,
            <a href='https://gael-varoquaux.info/' target='_blank' rel="noopener noreferrer">Gaël Varoquaux</a> and 
            <a href='https://www.mazenalamir.fr/' target='_blank' rel="noopener noreferrer">Alamir Mazen</a>. The final manuscprit is complete and publicly available <a href='https://theses.fr/2024UPASG046' target='_blank' rel="noopener noreferrer">here</a>.  
        </div>
    </div>

    <!-- RA-AD CIKM 2024-->

    <div class="row">
        <div class="col-xs-12 col-sm-2 col-md-2 date-column">
            <span class="label label-2024">07 / 2024</span>
        </div>
        <div class="col-xs-12 col-sm-10 col-md-10">
            Our paper <a href="https://arxiv.org/abs/2401.17052" target="_blank">Retrieval Augmented Deep Anomaly Detection for Tabular Data</a> 
            has been accepted at <a href="https://cikm2024.org/" target="_blank">CIKM 2024</a>. See you in Boise (USA)!
        </div>
    </div>

    <!-- NPT-AD ICML 2024-->

    <div class="row">
        <div class="col-xs-12 col-sm-2 col-md-2 date-column">
            <span class="label label-2024">05 / 2024</span>
        </div>
        <div class="col-xs-12 col-sm-10 col-md-10">
            Our paper <a href="https://arxiv.org/abs/2305.15121" target="_blank" rel="noopener noreferrer">Beyond Individual Input for Anomaly Detection on Tabular Data</a> 
            has been accepted at <a href="https://icml.cc/" target="_blank" rel="noopener noreferrer">ICML 2024</a>. See you in Vienna!
        </div>
    </div>

    <!-- Preprint NPT-AD -->

    <div class="row">
        <div class="col-xs-12 col-sm-2 col-md-2 date-column">
            <span class="label label-2023">05 / 2023</span>
        </div>
        <div class="col-xs-12 col-sm-10 col-md-10">
            Our preprint "<i>Beyond Individual Input for Anomaly Detection on Tabular Data</i>" 
            has been uploaded on <a href="https://arxiv.org/abs/2305.15121" target="_blank" rel="noopener noreferrer">arXiv</a>!
        </div>
    </div>

    <!-- IJCNN 2022 -->

    <div class="row">
        <div class="col-xs-12 col-sm-2 col-md-2 date-column">
            <span class="label label-2022">04 / 2022</span>
        </div>
        <div class="col-xs-12 col-sm-10 col-md-10">
            Our paper <a href="https://arxiv.org/abs/2205.01362" target="_blank" rel="noopener noreferrer">TracInAD: Measuring Influence for Anomaly Detection</a>
            has been accepted to <a
                href="https://wcci2022.org/" target="_blank" rel="noopener noreferrer">IJCNN 2022</a> for oral presentation!
        </div>
    </div>

    <!-- PhD 2021 -->

    <div class="row">
        <div class="col-xs-12 col-sm-2 col-md-2 date-column">
            <span class="label label-2021">09 / 2021</span>
        </div>
        <div class="col-xs-12 col-sm-10 col-md-10">
            I started my PhD in Computer Science at 
            <a href="https://www.centralesupelec.fr/fr/le-centre-de-recherche-centralesupelec" target="_blank" rel="noopener noreferrer">CentraleSupélec</a>
             and <a href="https://www.lisn.upsaclay.fr/" target="_blank" rel="noopener noreferrer">LISN</a>!
        </div>
    </div>

    <!-- ICME 2021 -->

    <div class="row">
        <div class="col-xs-12 col-sm-2 col-md-2 date-column">
            <span class="label label-2021">02 / 2021</span>
        </div>
        <div class="col-xs-12 col-sm-10 col-md-10">
            Our paper <a href="https://arxiv.org/abs/2103.07278" target="_blank" rel="noopener noreferrer">Learning Long-Term Style-Preserving Blind Video Temporal Consistency</a>
            has been accepted to <a
                href="http://2021.ieeeicme.org/2021.ieeeicme.org/index.html" target="_blank" rel="noopener noreferrer">ICME 2021</a> for oral presentation!
        </div>
    </div>

    <!-- L'Oreal RI internship -->

    <div class="row">
        <div class="col-xs-12 col-sm-2 col-md-2 date-column">
            <span class="label label-2020">06 / 2020</span>
        </div>
        <div class="col-xs-12 col-sm-10 col-md-10">
            I started a 6-month internship as a Deep Learning Scientist in the AI department 
            at <a href="https://www.loreal.com/fr/science-et-technologie-beaute/recherche-innovation-au-service-beaute/" target="_blank" rel="noopener noreferrer">L'Oreal Research & Innovation</a> under the
            supervision of <a href="https://scholar.google.com/citations?user=9fFK07IAAAAJ&hl=en" target="_blank" rel="noopener noreferrer">Julien Despois</a> and  <a
                href="https://scholar.google.fr/citations?user=RwyrWEkAAAAJ&hl=fr" target="_blank" rel="noopener noreferrer">Robin Kips</a>.
        </div>
    </div>
    <div style="height:3px;"></div>

    <!-- end of news -->
    <hr>


    <!-- Research -->
   <div class="row" id="research">
        <div class="col-md-12">
            <h2>Research</h2>

             <!-- EmoSLLM 2025 -->
             <div class="row">
                <div class="col-xs-10 col-sm-4 col-md-4">
                    <a class="thumbnail">
                        <img src="./content/media/emosllm_.png"
                             alt="EmoSLLM: Parameter-Efficient Adaptation of LLMs for Speech Emotion Recognition">
                    </a>
                </div>
                <div class="col-xs-12 col-sm-8 col-md-8">
                    <strong>EmoSLLM: Parameter-Efficient Adaptation of LLMs for Speech Emotion Recognition</strong> </br>
                    <u>Hugo Thimonier</u>, Antony Perzo, Renaud Seguier<br>
                    <em>NeurIPS Workshop on Multimodal Representation Learning for Healthcare, 2025.</em><br>
                    <a href="https://arxiv.org/abs/2508.14130" target="_blank" rel="noopener noreferrer"><button type="button" class="btn btn-primary btn-xs" rel="noopener noreferrer">pdf</button></a>
                    <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bibtex6">
                        bibtex
                    </button>
                    <div id="bibtex6" class="collapse">
                        <!-- Hidden message that will appear when BibTeX is shown -->
                        <div class="bibtex-message" style="display: none; color: grey; font-size: 12px;">
                            Click anywhere on the box bellow to highlight complete record.
                        </div>
                        <pre class="bibtex-pre"><tt>@inproceedings{thimonier2025emosllmparameterefficientadaptationllms,
    title={{EmoSLLM}: Parameter-Efficient Adaptation of {LLM}s for Speech Emotion Recognition}, 
    author={Hugo Thimonier and Antony Perzo and Renaud Seguier},
    year={2025},
    booktitle={NeurIPS Workshop on Multimodal Representation Learning for Healthcare},
    year={2025},
    url={https://openreview.net/forum?id=huMNytWWaq}
    }</tt></pre>
                    </div>
                    <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse"
                            data-target="#abstract6">abstract
                    </button>
                    <div id="abstract6" class="collapse">
                        <p style="text-align: justify;">
                            Emotion recognition from speech is a challenging task that requires 
                            capturing both linguistic and paralinguistic cues, with critical 
                            applications in human-computer interaction and mental health monitoring. 
                            Recent works have highlighted the ability of Large Language Models (LLMs) 
                            to perform tasks outside of the sole natural language area. In particular, 
                            recent approaches have investigated coupling LLMs with other data modalities 
                            by using pre-trained backbones and different fusion mechanisms. This work 
                            proposes a novel approach that fine-tunes an LLM with audio and text 
                            representations for emotion prediction. Our method first extracts audio 
                            features using an audio feature extractor, which are then mapped into 
                            the LLM's representation space via a learnable interfacing module. 
                            The LLM takes as input (1) the transformed audio features, 
                            (2) additional features in the form of natural language (e.g., the transcript), 
                            and (3) a textual prompt describing the emotion prediction task. 
                            To efficiently adapt the LLM to this multimodal task, we employ 
                            Low-Rank Adaptation (LoRA), enabling parameter-efficient fine-tuning. 
                            Experimental results on standard emotion recognition benchmarks 
                            demonstrate that our model outperforms all but one existing Speech-Text 
                            LLMs in the literature, while requiring less than half the parameters 
                            of competing approaches. This highlights our approach's effectiveness in 
                            integrating multi-modal inputs for speech-based emotion understanding 
                            while maintaining significant computational efficiency. 
                        </p>
                    </div>
                </div>
            </div>
            <!-- end of EmoSLLM 2025 -->

             <!-- ICLR 2025 -->
             <div class="row">
                <div class="col-xs-10 col-sm-4 col-md-4">
                    <a class="thumbnail">
                        <img src="./content/media/t_jepa.png"
                             alt="T-JEPA: Augmentation-Free Self-Supervised Learning for Tabular Data">
                    </a>
                </div>
                <div class="col-xs-12 col-sm-8 col-md-8">
                    <strong>T-JEPA: Augmentation-Free Self-Supervised Learning for Tabular Data</strong> </br>
                    <u>Hugo Thimonier</u>, José Lucas De Melo Costa, Fabrice Popineau, Arpad Rimmel and Bich-Liên Doan<br>
                    <em>Thirteenth International Conference on Learning Representations, ICLR 2025.</em><br>
                    <a href="https://arxiv.org/abs/2410.05016" target="_blank" rel="noopener noreferrer"><button type="button" class="btn btn-primary btn-xs" rel="noopener noreferrer">pdf</button></a>
                    <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bibtex5">
                        bibtex
                    </button>
                    <div id="bibtex5" class="collapse">
                        <!-- Hidden message that will appear when BibTeX is shown -->
                        <div class="bibtex-message" style="display: none; color: grey; font-size: 12px;">
                            Click anywhere on the box bellow to highlight complete record.
                        </div>
                        <pre class="bibtex-pre"><tt>@inproceedings{thimonier2025tjepa,
    title={T-JEPA: Augmentation-Free Self-Supervised Learning for Tabular Data},
    author={Hugo Thimonier and José Lucas De Melo Costa and Fabrice Popineau and Arpad Rimmel and Bich-Liên Doan},
    booktitle={The Thirteenth International Conference on Learning Representations},
    year={2025},
    url={https://openreview.net/forum?id=gx3LMRB15C}
    }</tt></pre>
                    </div>
                    <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse"
                            data-target="#abstract5">abstract
                    </button>
                    <div id="abstract5" class="collapse">
                        <p style="text-align: justify;">
                            Self-supervision is often used for pre-training to foster performance 
                            on a downstream task by constructing meaningful representations of samples.
                            Self-supervised learning (SSL) generally involves generating different 
                            views of the same sample and thus requires data augmentations that are 
                            challenging to construct for tabular data. This constitutes one of 
                            the main challenges of self-supervision for structured data. In the 
                            present work, we propose a novel augmentation-free SSL method for 
                            tabular data. Our approach, T-JEPA, relies on a Joint Embedding Predictive 
                            Architecture (JEPA) and is akin to mask reconstruction in the latent space. 
                            It involves predicting the latent representation of one subset of features 
                            from the latent representation of a different subset within the same sample, 
                            thereby learning rich representations without augmentations. We use our method 
                            as a pre-training technique and train several deep classifiers on the obtained 
                            representation. Our experimental results demonstrate a substantial improvement 
                            in both classification and regression tasks, outperforming models trained 
                            directly on samples in their original data space. Moreover, T-JEPA enables 
                            some methods to consistently outperform or match the performance of 
                            traditional methods likes Gradient Boosted Decision Trees. To understand why, 
                            we extensively characterize the obtained representations and show that 
                            T-JEPA effectively identifies relevant features for downstream tasks 
                            without access to the labels. Additionally, we introduce regularization 
                            tokens, a novel regularization method critical for training of 
                            JEPA-based models on structured data. 
                        </p>
                    </div>
                    <a href="https://github.com/jose-melo/t-jepa" target="_blank" rel="noopener noreferrer"
                    ><button type="button" class="btn btn-primary btn-xs">project page</button></a>
                </div>
            </div>
            <!-- end of TJEPA 2025 -->

             <!-- CIKM 2024 -->
             <div class="row">
                <div class="col-xs-10 col-sm-4 col-md-4">
                    <a class="thumbnail">
                        <img src="./content/media/miniature_making_param.png"
                             alt="Making Parametric Anomaly Detection on Tabular Data Non-Parametric Again">
                    </a>
                </div>
                <div class="col-xs-12 col-sm-8 col-md-8">
                    <strong>Retrieval Augmented Deep Anomaly Detection for Tabular Data</strong> </br>
                    <u>Hugo Thimonier</u>, Fabrice Popineau, Arpad Rimmel and Bich-Liên Doan<br>
                    <em>33rd ACM International Conference on Information and Knowledge Management, CIKM '24</em>.<br>
                    <a href="https://arxiv.org/abs/2401.17052" target="_blank"><button type="button" class="btn btn-primary btn-xs">pdf</button></a>
                    <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bibtex4">
                        bibtex
                    </button>
                    <div id="bibtex4" class="collapse">
                        <!-- Hidden message that will appear when BibTeX is shown -->
                        <div class="bibtex-message" style="display: none; color: grey; font-size: 12px;">
                            Click anywhere on the box bellow to highlight complete record.
                        </div>
                        <pre class="bibtex-pre"><tt>@inproceedings{thimonier2024making, 
    series={CIKM ’24},
    title={Retrieval Augmented Deep Anomaly Detection for Tabular Data},
    volume={35},
    url={http://dx.doi.org/10.1145/3627673.3679559},
    DOI={10.1145/3627673.3679559},
    booktitle={Proceedings of the 33rd ACM International Conference on Information and Knowledge Management},
    publisher={ACM},
    author={Thimonier, Hugo and Popineau, Fabrice and Rimmel, Arpad and Doan, Bich-Liên},
    year={2024},
    month=oct,
    pages={2250–2259},
    collection={CIKM ’24}
    }</tt></pre>
                    </div>
                    <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse"
                            data-target="#abstract4">abstract
                    </button>
                    <div id="abstract4" class="collapse">
                        <p style="text-align: justify;">
                            Deep learning for tabular data has garnered increasing attention in recent years,
                            yet employing deep models for structured data remains challenging. While these
                            models excel with unstructured data, their efficacy with structured data has 
                            been limited. Recent research has introduced retrieval-augmented models to 
                            address this gap, demonstrating promising results in supervised tasks such as
                            classification and regression. In this work, we investigate using retrieval-augmented
                            models for anomaly detection on tabular data. We propose a reconstruction-based 
                            approach in which a transformer model learns to reconstruct masked features 
                            of normal samples. We test the effectiveness of KNN-based and attention-based
                            modules to select relevant samples to help in the reconstruction process of the 
                            target sample. Our experiments on a benchmark of 31 tabular datasets reveal that 
                            augmenting this reconstruction-based anomaly detection (AD) method with sample-sample 
                            dependencies via retrieval modules significantly boosts performance. The present 
                            work supports the idea that retrieval module are useful to augment any deep 
                            AD method to enhance anomaly detection on tabular data.
                        </p>
                    </div>
                    <a href="https://github.com/hugothimonier/Making-Parametric-Anomaly-Detection-on-Tabular-Data-Non-Parametric-Again" target="_blank" rel="noopener noreferrer"
                    ><button type="button" class="btn btn-primary btn-xs">project page</button></a>
                </div>
            </div>
            <!-- end of CIKM 2024 -->

                    <!-- ICML 2024 -->
                    <div class="row">
                        <div class="col-xs-10 col-sm-4 col-md-4">
                            <a class="thumbnail">
                                <img src="./content/media/NPT-AD.png"
                                     alt="NPT-AD">
                            </a>
                        </div>
                        <div class="col-xs-12 col-sm-8 col-md-8">
                            <strong>Beyond Individual Input for Deep Anomaly Detection on Tabular Data</strong> </br>
                            <u>Hugo Thimonier</u>, Fabrice Popineau, Arpad Rimmel and Bich-Liên Doan<br>
                            <em>41st International Conference on Machine Learning, ICML 2024</em>.<br>
                            <a href="https://arxiv.org/pdf/2305.15121" target="_blank"><button type="button" class="btn btn-primary btn-xs">pdf</button></a>
                            <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bibtex3">
                                bibtex
                            </button>
                            <div id="bibtex3" class="collapse">
                                <!-- Hidden message that will appear when BibTeX is shown -->
                                <div class="bibtex-message" style="display: none; color: grey; font-size: 12px;">
                                    Click anywhere on the box bellow to highlight complete record.
                                </div>
                                <pre class="bibtex-pre"><tt>@InProceedings{pmlr-v235-thimonier24a,
    title = {Beyond Individual Input for Deep Anomaly Detection on Tabular Data},
    author = {Thimonier, Hugo and Popineau, Fabrice and Rimmel, Arpad and Doan, Bich-Li\^{e}n},
    booktitle = {Proceedings of the 41st International Conference on Machine Learning},
    pages = {48097--48123},
    year = {2024},
    editor = {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
    volume = {235},
    series = {Proceedings of Machine Learning Research},
    month =  {21--27 Jul},
    publisher = {PMLR},
    pdf = {https://raw.githubusercontent.com/mlresearch/v235/main/assets/thimonier24a/thimonier24a.pdf},
    url = {https://proceedings.mlr.press/v235/thimonier24a.html},
    }</tt></pre>
                            </div>
                            <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse"
                                    data-target="#abstract3">abstract
                            </button>
                            <div id="abstract3" class="collapse">
                                <p style="text-align: justify;">
                                    Anomaly detection is vital in many domains, such as finance, healthcare, 
                                    and cybersecurity. In this paper, we propose a novel deep anomaly 
                                    detection method for tabular data that leverages Non-Parametric 
                                    Transformers (NPTs), a model initially proposed for supervised tasks, 
                                    to capture both feature-feature and sample-sample dependencies. 
                                    In a reconstruction-based framework, we train the NPT to r
                                    econstruct masked features of normal samples. In a non-parametric fashion, 
                                    we leverage the whole training set during inference and use 
                                    the model's ability to reconstruct the masked features to generate 
                                    an anomaly score. To the best of our knowledge, this is the first work 
                                    to successfully combine feature-feature and sample-sample dependencies for 
                                    anomaly detection on tabular datasets. Through extensive experiments on 
                                    31 benchmark tabular datasets, we demonstrate that our method achieves 
                                    state-of-the-art performance, outperforming existing methods by 2.4% and 1.2%
                                    in terms of F1-score and AUROC, respectively. Our ablation study provides 
                                    evidence that modeling both types of dependencies is crucial for 
                                    anomaly detection on tabular data.
                                </p>
                            </div>
                            <a href="https://github.com/hugothimonier/NPT-AD" target="_blank" rel="noopener noreferrer"
                            ><button type="button" class="btn btn-primary btn-xs">project page</button></a>
                        </div>
                    </div>
                    <!-- end of NeurIPS 2023 -->


            <!-- IJCNN 2022 -->
            <div class="row">
                <div class="col-xs-10 col-sm-4 col-md-4">
                    <a class="thumbnail">
                        <img src="./content/media/tracinad.png"
                             alt="TracInAD">
                    </a>
                </div>
                <div class="col-xs-12 col-sm-8 col-md-8">
                    <strong>TracInAD: Measuring Influence for Anomaly Detection</strong> </br>
                    <u>Hugo Thimonier</u>, Fabrice Popineau, Arpad Rimmel, Bich-Liên Doan and Fabrice Daniel<br>
                    <em>International Joint Conference on Neural Networks, IJCNN 2022</em>.<br>
                    <a href="https://arxiv.org/abs/2205.01362" target="_blank"><button type="button" class="btn btn-primary btn-xs">pdf</button></a>
                    <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bibtex2">
                        bibtex
                    </button>
                    <div id="bibtex2" class="collapse">
                    <!-- Hidden message that will appear when BibTeX is shown -->
                    <div class="bibtex-message" style="display: none; color: grey; font-size: 12px;">
                        Click anywhere on the box bellow to highlight complete record.
                    </div>
                    <pre class="bibtex-pre"><tt>@inproceedings{thimonier2022,
    author={Thimonier, Hugo and Popineau, Fabrice and Rimmel, Arpad and Doan, Bich-Liên and Daniel, Fabrice},
    booktitle={2022 International Joint Conference on Neural Networks (IJCNN)},
    title={{TracInAD}: Measuring Influence for Anomaly Detection}, 
    year={2022},
    volume={},
    number={},
    pages={1-6},
    doi={10.1109/IJCNN55064.2022.9892058}
    }</tt></pre>
                    </div>
                    <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse"
                            data-target="#abstract2">abstract
                    </button>
                    <div id="abstract2" class="collapse">
                        <p style="text-align: justify;">
                            As with many other tasks, neural networks prove
                            very effective for anomaly detection purposes. However, very
                            few deep-learning models are suited for detecting anomalies on
                            tabular datasets. This paper proposes a novel methodology to
                            flag anomalies based on TracIn, an influence measure initially
                            introduced for explicability purposes. The proposed methods
                            can serve to augment any unsupervised deep anomaly detection
                            method. We test our approach using Variational Autoencoders
                            and show that the average influence of a subsample of training
                            points on a test point can serve as a proxy for abnormality. Our
                            model proves to be competitive in comparison with state-of-the-
                            art approaches: it achieves comparable or better performance
                            in terms of detection accuracy on medical and cyber-security
                            tabular benchmark data.
                        </p>
                    </div>
                    <a href="https://github.com/TracInAD/TracInAD" target="_blank" rel="noopener noreferrer"><button type="button" class="btn btn-primary btn-xs">project page</button></a>
                </div>
            </div>
            <!-- end of arXiv'21 -->


            <!-- ICME 2021 -->
            <div class="row">
                <div class="col-xs-10 col-sm-4 col-md-4">
                    <a class="thumbnail">
                        <img src="./content/media/icme2021_miniature.PNG"
                             alt="Learning Long Term Style Preserving Blind Video Temporal Consistency">
                    </a>
                </div>
                <div class="col-xs-12 col-sm-8 col-md-8">
                    <strong>Learning Long Term Style Preserving Blind Video Temporal Consistency</strong> </br>
                    <u>Hugo Thimonier</u>, Julien Despois, Robin Kips, Matthieu Perrot<br>
                    <em>IEEE International Conference on Multimedia and Expo, ICME 2021</em>.<br>
                    <a href="https://arxiv.org/abs/2103.07278" target="_blank" rel="noopener noreferrer"><button type="button" class="btn btn-primary btn-xs">pdf</button></a>
                    <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bibtex1">
                        bibtex
                    </button>
                    <div id="bibtex1" class="collapse">
                        <!-- Hidden message that will appear when BibTeX is shown -->
                        <div class="bibtex-message" style="display: none; color: grey; font-size: 12px;">
                            Click anywhere on the box bellow to highlight complete record.
                        </div>
                        <pre class="bibtex-pre"><tt>@inproceedings{thimonier2021,
        author={Thimonier, Hugo and Despois, Julien and Kips, Robin and Perrot, Matthieu},
        booktitle={2021 IEEE International Conference on Multimedia and Expo (ICME)}, 
        title={Learning Long Term Style Preserving Blind Video Temporal Consistency}, 
        year={2021},
        volume={},
        number={},
        pages={1-6},
        doi={10.1109/ICME51207.2021.9428445}
        }</tt></pre>
                    </div>
                    <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse"
                            data-target="#abstract1">abstract
                    </button>
                    <div id="abstract1" class="collapse">
                        <p style="text-align: justify;">
                            When trying to independently apply image-trained algorithms
                            to successive frames in videos, noxious flickering tends to appear. 
                            State-of-the-art post-processing techniques that aim at
                            fostering temporal consistency, generate other temporal artifacts
                            and visually alter the style of videos. We propose a post-
                            processing model, agnostic to the transformation applied to
                            videos (e.g. style transfer, image manipulation using GANs,
                            etc.), in the form of a recurrent neural network. Our model
                            is trained using a Ping Pong procedure and its corresponding 
                            loss, recently introduced for GAN video generation, as
                            well as a novel style preserving perceptual loss. The former
                            improves long-term temporal consistency learning, while the
                            latter fosters style preservation. We evaluate our model on the
                            DAVIS and videvo.net datasets and show that our approach
                            offers state-of-the-art results concerning flicker removal, and
                            better keeps the overall style of the videos than previous approaches.
                        </p>
                    </div>
                    <a href="https://hugothimonier.github.io/StylePreservingTimeConsistency/" target="_blank" rel="noopener noreferrer"><button type="button" class="btn btn-primary btn-xs">project page</button></a>
                </div>
            </div>
            <!-- end of ICME 21 -->

        </div>
    </div> <!-- end of projects -->


    <hr>

    <div class="container">
        <footer>
            <p align="right"><small>Copyright © Hugo Thimonier &nbsp;/&nbsp; Last update: 
                <span id="lastUpdate"></span></small></p>
        </footer>
        <div style="height:10px;"></div>
    </div>

    <!-- Bootstrap core JavaScript -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="js/jquery-1.11.1.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script src="js/docs.min.js"></script>

    <!-- JavaScript for selecting the BibTeX and showing the message -->
    <script src="js/copy.min.js"></script>

    <script src="js/scale.fix.js"></script>

    <script>
        // Get the last modified date of the document
        const lastModified = new Date(document.lastModified);

        // Create a date options object to format the date as Month YYYY
        const options = { year: 'numeric', month: 'long' };

        // Insert the formatted date into the span with ID 'lastUpdate'
        document.getElementById('lastUpdate').textContent = lastModified.toLocaleDateString('en-US', options);
    </script>


</main>
</body>
</html>